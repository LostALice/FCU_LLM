# dev env for debug only

FROM python:3.11

RUN apt-get update && apt-get install -y


# install cuda
RUN wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda-repo-debian12-12-4-local_12.4.1-550.54.15-1_amd64.deb
RUN dpkg -i cuda-repo-debian12-12-4-local_12.4.1-550.54.15-1_amd64.deb
RUN cp /var/cuda-repo-debian12-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/
# RUN add-apt-repository contrib
RUN apt-get update
RUN apt-get -y install cuda-toolkit-12-4

# install llama-cpp-python

# Warnings: please read the docs https://llama-cpp-python.readthedocs.io/en/stable/ before using cuda
# --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/<cuda-version>
# cuda:12.3 = --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123
RUN CUDACXX=/usr/local/cuda-12/bin/nvcc CMAKE_ARGS="-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all-major" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --force-reinstall --upgrade --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123

# install required libraries
RUN mkdir /FCU_LLM
WORKDIR /FCU_LLM
ADD ./req.txt .
RUN pip install -r ./req.txt

ADD . .

RUN mkdir files
RUN mkdir model

ENTRYPOINT ["uvicorn", "--host", "0.0.0.0", "main:app", "--port", "8080", "--reload"]